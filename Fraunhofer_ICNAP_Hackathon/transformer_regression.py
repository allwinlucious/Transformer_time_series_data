# -*- coding: utf-8 -*-
"""transformer_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-KaTL3CHwajy61GVnCJ0ofpNkJLvWiQi
"""

pip install keras_nlp

from dataclasses import dataclass
import os
import numpy as np
from sklearn.model_selection import train_test_split
# Import packages as you need
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from keras_nlp.layers import SinePositionEncoding
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay

"""Load Data"""

df = pd.read_json("/content/drive/MyDrive/Colab Notebooks/ipt_12.json")

"""Data Exploration

"""

print(df.shape)
df.info()

np.unique(df.material)

target = df.VB
print("target min = ",min(target))
print("target max = ",max(target))
print("target mean = ",np.mean(target))
print("target variance = ",np.var(target))

print(df.corr(method = "spearman"))

"""its observed that VB and run are highly correlated . Time and run is also highly correlated, hence dimensionality reduction could be applied."""

plt.plot(df.VB,df.run)

plt.plot(df.time,df.run)

df[["run","VB"]]

"""removing all rows for which VB == NAN ##or these could be interpolated"""

df = df.dropna()
df = df.reset_index()
print(df.shape)

"""**TRY DIMENSIONALITY REDUCTION**"""

df.columns

"""separate the time series data

"""

tsdf = df[['smcAC',
       'smcDC', 'vib_table', 'vib_spindle', 'AE_table', 'AE_spindle']]

X = np.zeros((146,9000,6))
i=0
for col in ['smcAC','smcDC', 'vib_table', 'vib_spindle', 'AE_table', 'AE_spindle']:
  X[:,:,i] = np.array(tsdf[col].to_list())
  i += 1

y = np.array(df.VB)
y.shape

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    x_ = SinePositionEncoding()(x)
    x= x + x_
    
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format="channels_first")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(1)(x) #no activation
    return keras.Model(inputs, outputs)

def model_training(train):
    """
    Train the data with the compatible model
    """
    input_shape = X_train.shape[1:]
    model=build_model(
                        input_shape,
                        head_size=256,
                        num_heads=4,
                        ff_dim=4,
                        num_transformer_blocks=4,
                        mlp_units=[128],
                        mlp_dropout=0.4,
                        dropout=0.25,
                     )
    model.compile(
                    loss='mean_squared_error',
                    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
                    metrics=keras.metrics.MeanSquaredError(),
    )
    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]
    
    model.fit(
                X_train,
                Y_train,
                validation_split=0.2,
                epochs=200,
                batch_size=16,
                callbacks=callbacks,
    )
    
    
    return model



# def metric(validation_data, model):
#     """
#     Standard metrics and plotting should be same
#     Metrics should be computed on validation data(unseen data)
#     1. Balanced accuracy score
#     2. Confusion matrix
#     3. Per-class accuracy
#     """
#     X_val, Y_val = validation_data
     #Y_pred = model.predict(X_val)> 0.5
#     ba=balanced_accuracy_score(Y_val, Y_pred)
#     cm=confusion_matrix(Y_val, Y_pred)
#     # cm_display = ConfusionMatrixDisplay(cm).plot()
#     metrics=[cm,ba]
#     return metrics

# def validation(metrics, metrics_validation):
#     """
#     Comparing the results with provided Series Embedder
#     Plot confusion matrices of self analysis and LSTM with balanced_accuracy
    
#     """
#     cm_model = ConfusionMatrixDisplay(metrics[0]).plot()
#     cm_lstm = ConfusionMatrixDisplay(metrics_validation[0]).plot()

    # metrics=metric(val,model_self)
    
#     lstm_cm,lstm_balanced_accuracy=lstm(preprocessed_data,target='labels')
#     metrics_validation = [lstm_cm, lstm_balanced_accuracy]
#     validation(metrics,metrics_validation)

model_self=model_training((X_train,Y_train))

